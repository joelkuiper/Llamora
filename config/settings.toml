[default]
APP_NAME = "Llamora"
# Development default; replace with your own value in production.
SECRET_KEY = "O7T_mOBS765PydhqWAhH1RArhWBEp6hu28Uhf2gN0Xs"
LOG_LEVEL = "INFO"

[default.APP]
host = "127.0.0.1"
port = 5000

[default.FEATURES]
disable_registration = false

[default.LIMITS]
max_tag_length = 64
max_username_length = 30
max_password_length = 128
min_password_length = 8
max_message_length = 12000
max_search_query_length = 512

[default.TAG_RECALL]
summary_sentences = 6
summary_max_chars = 1200
history_scope = 20
max_tags = 6
max_snippets = 6

[default.SEARCH]
recent_limit = 50
recent_suggestion_limit = 8
entry_index_max_elements = 100000
page_size = 10
initial_page_size = 15
result_window = 100
stream_ttl = 900
stream_max_sessions = 200
stream_debug_delay_ms = 0

[default.SEARCH.progressive]
k1 = 128
k2 = 10
rounds = 3
batch_size = 1000
max_ms = 1500
poor_match_max_cos = 0.28
poor_match_min_hits = 3

[default.AUTH]
max_login_attempts = 5
login_lockout_ttl = 900
login_failure_cache_size = 2048

[default.SESSION]
ttl = 604800

[default.MESSAGES.history_cache]
maxsize = 256
ttl = 60

[default.DATABASE]
path = "state.sqlite3"
pool_size = 25
pool_acquire_timeout = 10
timeout = 5.0
busy_timeout = 5000
mmap_size = 10485760

[default.EMBEDDING]
model = "BAAI/bge-small-en-v1.5"
concurrency = 0

[default.LLM.upstream]
llamafile_path = ""
# When set, connect to an existing OpenAI-compatible endpoint (e.g., http://127.0.0.1:8081 from
# `llama-server --hf Qwen/Qwen3-4B-Instruct-2507 --jinja`).
host = ""
parallel = 1

[default.LLM.upstream.args]
server = true
nobrowser = true
threads = 0
n_gpu_layers = 999
gpu = "auto"
ctx_size = 8192

[default.LLM.generation]
n_predict       = 1024
stream          = true
stop            = ["<|im_start|>", "<|im_end|>", "<|endoftext|>", "<|end|>"]
n_keep          = -1
cache_prompt    = true

mirostat        = 2
mirostat_tau    = 4.5
mirostat_eta    = 0.12

repeat_penalty  = 1.25
repeat_last_n   = 320
penalize_nl     = true

temperature = 0.7
top_p = 0.8
top_k = 20

[default.LLM.stream]
pending_ttl = 300
queue_limit = 4
repeat_guard_size = 6
repeat_guard_min_length = 12

[default.LLM]
allowed_config_keys = ["temperature"]
response_kinds = [
  { id = "reply", label = "Reply", prompt = "Generate a direct continuation of the thread, responding normally to the content." },
  { id = "acknowledge", label = "Acknowledge", prompt = "Generate a supportive, validating response that focuses on reassurance, empathy, or positive acknowledgment. Do not introduce new analysis or advice unless clearly implied." },
  { id = "notice", label = "Notice", prompt = "Generate an observational response that points out patterns, themes, or notable aspects of the message, without judgment or encouragement. Keep it descriptive rather than reactive." },
]

[default.UI]
clock_format = "24h"

[default.LLM.tokenizer]
encoding = "cl100k_base"

[default.LLM.tokenizer.safety_margin]
ratio = 0.1
min_tokens = 128

[default.LLM.chat]
endpoint = "/v1/chat/completions"
model = "local"
# base_url = "http://127.0.0.1:8081/v1"
# api_key = ""
parameter_allowlist = [
  "top_k",
  "n_keep",
  "cache_prompt",
  "mirostat",
  "mirostat_tau",
  "mirostat_eta",
  "repeat_penalty",
  "repeat_last_n",
  "penalize_nl",
]
parameters = {}

[default.PROMPTS]
template_dir = "../src/llamora/llm/templates"

[default.COOKIES]
name = "llamora"
# Development default; replace with your own value in production.
secret = "K3aRvB6X6lXe9iWvL99XQqSlqSkJ3YTaxqz81uB_ihw="

[default.CRYPTO]
dek_storage = "cookie"

[default.WORKERS.index_worker]
max_queue_size = 1024
batch_size = 32
flush_interval = 0.05

[development]
DEBUG = true

[production]
DEBUG = false
